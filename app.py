import streamlit as st
import os
from dotenv import load_dotenv
import google.generativeai as genai
import chromadb
from chromadb.config import Settings
from pypdf import PdfReader # PDF okumak iÃ§in yeni kÃ¼tÃ¼phanemiz

# --- SAYFA AYARLARI ---
st.set_page_config(page_title="PDF AsistanÄ±", layout="wide")

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")

# Anahtar kontrolÃ¼
if not API_KEY:
    st.error("âŒ LÃ¼tfen .env dosyasÄ±na GOOGLE_API_KEY ekleyin!")
    st.stop()

genai.configure(api_key=API_KEY)

# Modeller
EMBEDDING_MODEL = "models/text-embedding-004"
CHAT_MODEL = "models/gemini-2.5-flash"

# --- FONKSÄ°YONLAR ---

def get_chroma_client():
    # ChromaDB'yi Ã¶nbelleÄŸe almadan her seferinde taze Ã§aÄŸÄ±rÄ±yoruz
    return chromadb.Client(Settings(is_persistent=False))

def pdf_to_text(uploaded_file):
    """PDF dosyasÄ±ndan metinleri Ã§Ä±karÄ±r"""
    text = ""
    pdf_reader = PdfReader(uploaded_file)
    for page in pdf_reader.pages:
        text += page.extract_text() + "\n"
    return text

def chunk_text(text, chunk_size=1000, overlap=100):
    """Metni yapay zekanÄ±n yiyebileceÄŸi kÃ¼Ã§Ã¼k parÃ§alara bÃ¶ler"""
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += end - overlap # BaÄŸlam kopmasÄ±n diye biraz geriden al
    return chunks

# --- ARAYÃœZ ---

st.title("ğŸ“„ PDF DosyasÄ±yla KonuÅŸ")
st.markdown("Yanda bir PDF yÃ¼kleyin ve yapay zekanÄ±n onu Ã¶ÄŸrenmesini izleyin!")

# Sol MenÃ¼ (Sidebar)
with st.sidebar:
    st.header("ğŸ“ DokÃ¼man YÃ¼kle")
    uploaded_file = st.file_uploader("Bir PDF dosyasÄ± seÃ§in", type="pdf")
    
    if uploaded_file:
        if st.button("ğŸ§  Yapay Zekaya Ã–ÄŸret"):
            with st.spinner("PDF okunuyor ve analiz ediliyor..."):
                try:
                    # 1. Metni Oku
                    raw_text = pdf_to_text(uploaded_file)
                    st.info(f"PDF Okundu! Toplam {len(raw_text)} karakter.")
                    
                    # 2. ParÃ§alara BÃ¶l
                    text_chunks = chunk_text(raw_text)
                    st.write(f"ğŸ§© Metin {len(text_chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼.")
                    
                    # 3. VeritabanÄ±nÄ± HazÄ±rla
                    client = get_chroma_client()
                    try:
                        client.delete_collection("pdf_hafiza")
                    except:
                        pass
                    collection = client.create_collection(name="pdf_hafiza")
                    
                    # 4. Embedding (VektÃ¶re Ã‡evir) ve Kaydet
                    ids = [str(i) for i in range(len(text_chunks))]
                    
                    embeddings = []
                    # Ä°lerleme Ã§ubuÄŸu
                    progress_bar = st.progress(0)
                    
                    for i, chunk in enumerate(text_chunks):
                        emb = genai.embed_content(
                            model=EMBEDDING_MODEL,
                            content=chunk,
                            task_type="retrieval_document"
                        )['embedding']
                        embeddings.append(emb)
                        progress_bar.progress((i + 1) / len(text_chunks))
                    
                    collection.add(documents=text_chunks, embeddings=embeddings, ids=ids)
                    
                    st.session_state['db_ready'] = True
                    st.session_state['collection'] = collection
                    st.success("âœ… Ã–ÄŸrenme TamamlandÄ±! ArtÄ±k soru sorabilirsin.")
                    
                except Exception as e:
                    st.error(f"Bir hata oluÅŸtu: {e}")

# --- SOHBET ALANI ---

if "messages" not in st.session_state:
    st.session_state.messages = []

# Mesaj geÃ§miÅŸini gÃ¶ster
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ± soru sorarsa
if prompt := st.chat_input("PDF hakkÄ±nda ne bilmek istersin?"):
    # EÄŸer dosya yÃ¼klenmediyse uyar
    if 'db_ready' not in st.session_state:
        st.error("âš ï¸ Ã–nce soldan bir PDF yÃ¼kleyip 'Ã–ÄŸret' butonuna basmalÄ±sÄ±n!")
    else:
        # MesajÄ± gÃ¶ster
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("AraÅŸtÄ±rÄ±yorum...")
            
            try:
                # 1. VektÃ¶r Arama
                collection = st.session_state['collection']
                
                soru_vektoru = genai.embed_content(
                    model=EMBEDDING_MODEL,
                    content=prompt,
                    task_type="retrieval_query"
                )['embedding']
                
                # En alakalÄ± 3 parÃ§ayÄ± getir (Daha kapsamlÄ± cevap iÃ§in)
                arama_sonucu = collection.query(query_embeddings=[soru_vektoru], n_results=3)
                
                # Bulunan parÃ§alarÄ± birleÅŸtir
                context_text = "\n\n".join(arama_sonucu['documents'][0])
                
                # 2. Gemini'ye GÃ¶nder
                full_prompt = f"""
                Sen uzman bir dokÃ¼man analistisin. AÅŸaÄŸÄ±daki PDF iÃ§eriÄŸini kullanarak soruyu cevapla.
                
                PDF Ä°Ã‡ERÄ°ÄÄ°NDEN PARÃ‡ALAR:
                {context_text}
                
                SORU: {prompt}
                
                CevabÄ± verirken sadece PDF'teki bilgileri kullan. Bilgi yoksa "DokÃ¼manda bu bilgi geÃ§miyor" de.
                """
                
                model = genai.GenerativeModel(CHAT_MODEL)
                response = model.generate_content(full_prompt)
                
                # CevabÄ± yaz
                message_placeholder.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
                
                # KaynaklarÄ± gÃ¶ster (Opsiyonel)
                with st.expander("Hangi parÃ§alara baktÄ±m?"):
                    st.write(context_text)
                    
            except Exception as e:
                message_placeholder.error(f"Hata: {e}")